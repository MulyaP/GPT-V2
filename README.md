# GPT V2

This is an encoder-decoder transformer model that I trained on openwebtext corpus. Instead of using character level tokenizer like in V1, I used a pretrained tokenizer of GPT-2, that is, the Byte-pair encoding. Installation steps can be found [here]{https://github.com/MulyaP/GPT/blob/main/README.md}
