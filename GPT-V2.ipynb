{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import mmap\n",
    "import random\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Set the device as cuda if cuda is available\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 64  # Size of the input sequence \n",
    "batch_size = 32  # Amount of inputs the model would train on in one single iteration\n",
    "max_iters = 300\n",
    "lr = 3e-4  # Learning rate (defines the rate with which the model would converge)\n",
    "eval_iters = 10\n",
    "# eval_interval = 500\n",
    "n_embd = 200  # Number of embedding tokens generated per input character\n",
    "n_layers = 1   # Number of decoder layers\n",
    "n_head = 1  # Number of attention layers running in parallel\n",
    "dropout = 0.2  # dropout to prevent overfitting\n",
    "max_token_size = 64\n",
    "EOS_Token =50256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_random_chunk(split):\n",
    "    filename = \"openwebtext/train.txt\" if split=='train' else \"openwebtext/val.txt\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "            file_size = len(mm)\n",
    "            start_pos = random.randint(0, (file_size) - block_size*batch_size*max_token_size)\n",
    "\n",
    "            mm.seek(start_pos)\n",
    "            block = mm.read(block_size*batch_size*max_token_size)\n",
    "\n",
    "            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '')\n",
    "            \n",
    "            decoded_block = decoded_block[decoded_block.find(\" \")+1:decoded_block.rfind(\" \")]\n",
    "            # print(decoded_block)\n",
    "            data = torch.tensor(tokenizer.encode(decoded_block), dtype=torch.long)\n",
    "            \n",
    "    return data\n",
    "\n",
    "def get_batch(split):\n",
    "    data = get_random_chunk(split)\n",
    "    # Fetches a random chunk of data of size (batch_size*block_size) from the whole dataset\n",
    "    ix = torch.randint(len(data)-block_size,(batch_size, ))  # Randomly generates starting positions of the input sequences for a whole batch\n",
    "    # print(ix)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])  # For every starting position, it generates input sequence\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# x,y = get_batch('train')\n",
    "# print(x.shape)\n",
    "# print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x,y = get_batch('train')\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdecode\u001b[49m(x[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mtolist()))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(decode(y[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mtolist()))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'decode' is not defined"
     ]
    }
   ],
   "source": [
    "print(decode(x[0][1:].tolist()))\n",
    "print(decode(y[0][1:].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))  # Lower triangular mask is used because for each timestep, we don't want the model to learn from the future timesteps\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, key, value, query, masked=None):\n",
    "        B,T,C = query.shape\n",
    "        k = self.key(key)\n",
    "        q = self.query(query)\n",
    "        v = self.value(value)\n",
    "\n",
    "        sim_score = torch.matmul(q,k.transpose(-2,-1))*k.shape[-1]**0.5\n",
    "        if masked is not None: \n",
    "            sim_score = sim_score.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
    "        sim_score = F.softmax(sim_score, dim=-1)\n",
    "        sim_score = self.dropout(sim_score)\n",
    "        attention_score = torch.matmul(sim_score,v)\n",
    "        return attention_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_head, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])\n",
    "        self.proj = nn.Linear(head_size*n_head, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, key, value, query, masked=None):\n",
    "        out = torch.cat([h(key, value, query, masked) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.MA1 = MultiHeadAttention(n_head, head_size)\n",
    "        self.MA2 = MultiHeadAttention(n_head, head_size)\n",
    "        self.ff = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.ln3 = nn.LayerNorm(n_embd)\n",
    "\n",
    "\n",
    "    def forward(self, x, encoder_output):\n",
    "        y = self.MA1(x, x, x, masked=True)\n",
    "        x = self.ln1(x + y)\n",
    "        y = self.MA2(encoder_output, encoder_output, x)\n",
    "        x = self.ln2(x + y)\n",
    "        y = self.ff(x)\n",
    "        x = self.ln2(x + y)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.MA1 = MultiHeadAttention(n_head, head_size)\n",
    "        self.ff = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # print(x.dtype)\n",
    "        y = self.MA1(x, x, x)\n",
    "        x = self.ln1(x + y)\n",
    "        y = self.ff(x)\n",
    "        x = self.ln2(x + y)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings_and_positional_encoding(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.input_embd_layer = nn.Embedding(vocab_size, n_embd)\n",
    "        self.input_positional_encoding = nn.Embedding(block_size, n_embd)\n",
    "    def forward(self, index):\n",
    "        y = self.input_embd_layer(index)\n",
    "        B, T, C = y.shape\n",
    "        p = self.input_positional_encoding(torch.arange(T, device=device))\n",
    "        return (y + p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire decoder-only transformer model with multi-head attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings_and_positional_encoding(vocab_size)\n",
    "        # self.output_embeddings = Embeddings_and_positional_encoding(vocab_size)\n",
    "        self.encoder = nn.Sequential(*[Encoder(vocab_size) for _ in range(n_layers)])\n",
    "        self.decoder = nn.ModuleList([Decoder(vocab_size) for _ in range(n_layers)])\n",
    "        self.ff = nn.Linear(n_embd, vocab_size)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    \n",
    "    def forward(self, inputs, targets=None):\n",
    "        input_embd = self.embeddings(inputs)\n",
    "        target_embd = self.embeddings(targets)\n",
    "        enc = self.encoder(input_embd)\n",
    "        dec = target_embd\n",
    "        for layer in self.decoder:\n",
    "            dec = layer(dec, enc)\n",
    "        x = self.ff(dec)\n",
    "        B, T, C = x.shape\n",
    "        x = F.softmax(x, dim=-1)\n",
    "        x = x.view(B*T, C)\n",
    "        y = targets.view(B*T)\n",
    "        loss = F.cross_entropy(x, y)\n",
    "        return x, loss\n",
    "        \n",
    "\n",
    "    def generate(self,prompt, max_new_tokens):\n",
    "        index = torch.tensor(tokenizer.encode(prompt), device=device)\n",
    "        index = index.view(1,-1)\n",
    "        targets = index.clone()\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            index = index[:, max(0,len(index[0])-block_size):]\n",
    "            targets = targets[:, max(0,len(targets[0])-block_size):]\n",
    "            logits, loss = self.forward(index, targets)\n",
    "            probs = logits[-1,:]\n",
    "            print(probs)\n",
    "            \n",
    "            index_next = torch.argmax(probs)\n",
    "            print(index_next)\n",
    "            index_next = index_next.view(1,-1)\n",
    "            targets = torch.cat((targets,index_next), dim=1)\n",
    "            if targets[0][-1]==EOS_Token:\n",
    "                return targets\n",
    "            # print(index.shape)\n",
    "        targets = torch.cat((targets, torch.tensor([[EOS_Token, ], ], device=device)), dim=1)\n",
    "        return targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train','val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits,loss = model(X,Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training function and model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0: train_loss : 10.824899673461914, eval_loss : 10.824899673461914\n",
      "iter: 10: train_loss : 10.82489013671875, eval_loss : 10.82489013671875\n",
      "iter: 20: train_loss : 10.82483959197998, eval_loss : 10.82483959197998\n",
      "iter: 30: train_loss : 10.824641227722168, eval_loss : 10.8246431350708\n",
      "iter: 40: train_loss : 10.824097633361816, eval_loss : 10.82406997680664\n",
      "iter: 50: train_loss : 10.822344779968262, eval_loss : 10.822399139404297\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 20\u001b[0m\n\u001b[0;32m     16\u001b[0m         torch\u001b[38;5;241m.\u001b[39msave(checkpoint, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_checkpoint.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# print(f\"iter: {iter}: train_loss : {cummulative_loss/(iter+1)}\")\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m x,y \u001b[38;5;241m=\u001b[39m \u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# print(x.dtype, y.dtype)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(x,y)\n",
      "Cell \u001b[1;32mIn[44], line 26\u001b[0m, in \u001b[0;36mget_batch\u001b[1;34m(split)\u001b[0m\n\u001b[0;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([data[i:i\u001b[38;5;241m+\u001b[39mblock_size] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])  \u001b[38;5;66;03m# For every starting position, it generates input sequence\u001b[39;00m\n\u001b[0;32m     25\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([data[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:i\u001b[38;5;241m+\u001b[39mblock_size\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\n\u001b[1;32m---> 26\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, y\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel(vocab_size).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "cummulative_loss = 0\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter%eval_iters==0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"iter: {iter}: train_loss : {losses['train']}, eval_loss : {losses['val']}\")\n",
    "        if iter%100==0:\n",
    "            checkpoint = {\n",
    "                'epoch': iter,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': losses['val'],  # Example: saving the last recorded loss\n",
    "            }\n",
    "            torch.save(checkpoint, 'model_checkpoint.pth')\n",
    "        # print(f\"iter: {iter}: train_loss : {cummulative_loss/(iter+1)}\")\n",
    "    \n",
    "    \n",
    "    x,y = get_batch('train')\n",
    "    # print(x.dtype, y.dtype)\n",
    "    logits, loss = model.forward(x,y)\n",
    "    cummulative_loss = cummulative_loss + loss.item()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing and loading the model from model_checkpoint.pth file. (Can be used to load a previously trained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8249)\n"
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel(vocab_size).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "checkpoint = torch.load('model_checkpoint.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['val_loss']\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating some sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a prompt:  Hi \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.7066e-05, 1.9260e-05, 1.8087e-05,  ..., 1.2694e-05, 1.3832e-05,\n",
      "        2.2234e-05], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor(29707, device='cuda:0')\n",
      "tensor([4.2430e-05, 1.6526e-05, 2.6421e-05,  ..., 2.2075e-05, 3.3479e-05,\n",
      "        1.4234e-05], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor(42346, device='cuda:0')\n",
      "tensor([2.0147e-05, 2.0265e-05, 1.3422e-05,  ..., 1.4402e-05, 2.7430e-05,\n",
      "        1.3203e-05], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor(17647, device='cuda:0')\n",
      "tensor([1.2580e-05, 2.0333e-05, 1.2956e-05,  ..., 2.6551e-05, 1.4052e-05,\n",
      "        2.6707e-05], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor(15919, device='cuda:0')\n",
      "tensor([2.0567e-05, 1.8615e-05, 1.3453e-05,  ..., 1.4808e-05, 2.4273e-05,\n",
      "        1.2935e-05], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor(31684, device='cuda:0')\n",
      "Hi  pens BART BirthOUND Liam\n"
     ]
    }
   ],
   "source": [
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "# encoded_prompt = torch.tensor([[SOS_Token, ], ], dtype=torch.long, device=device)\n",
    "prompt = input(\"Enter a prompt: \")\n",
    "# print(prompt)\n",
    "# prompt = torch.tensor(prompt, dtype=torch.long, device=device)\n",
    "# prompt = torch.cat((prompt, torch.tensor([EOS_Token], dtype=torch.long, device=device)))\n",
    "# prompt = prompt.view(1,-1)\n",
    "\n",
    "# encoded_prompt = torch.cat((encoded_prompt, prompt), dim=1)\n",
    "# print(encoded_prompt)\n",
    "encoded_output = model.generate(prompt, 5)[0].tolist()\n",
    "encoded_output = encoded_output[:len(encoded_output)-1]\n",
    "output = tokenizer.decode(encoded_output)\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
